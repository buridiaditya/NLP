{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Assignment Task 1 \n",
    "\n",
    "Neural Network - discriminative\n",
    "\n",
    "Naive Bayes classifier - generative\n",
    "\n",
    "Logistic regression - discriminative\n",
    "\n",
    "Gaussian mixture model - generative\n",
    "\n",
    "GANs - generative\n",
    "\n",
    "LDA - generative\n",
    "\n",
    "SVM - discriminative\n",
    "\n",
    "Decision Tree - discriminative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Assignment Task 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n",
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/grid_search.py:14: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, namedtuple, Sized\n",
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank,brown\n",
    "import math\n",
    "import scipy\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57240\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "corpus = brown.tagged_sents(tagset='universal')\n",
    "\n",
    "# Last 100 examples are used as test cases\n",
    "train_data = corpus[:-100] \n",
    "test_data = corpus[-10:]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM class\n",
    "\n",
    "class HMM():\n",
    "    def __init__(self,laplace_constant=0.5):\n",
    "        self.P = {}\n",
    "        self.O = {}\n",
    "        self.S = {}\n",
    "        self.laplace_constant = 0.5\n",
    "\n",
    "    def train(self,train_data):\n",
    "        \n",
    "        # Assignment Task 2 - Sub Part 1 creating transition, start, emission matrices\n",
    "        # self.P - transition matix\n",
    "        # self.O - emission matrix\n",
    "        # self.S - start matrix\n",
    "        \n",
    "        self.tag_dict={}\n",
    "        self.word_dict={}\n",
    "        total = 0\n",
    "\n",
    "        for sent in train_data:\n",
    "            for elem in sent:\n",
    "                w = elem[0]\n",
    "                tag= elem[1]\n",
    "\n",
    "                if w not in self.word_dict:\n",
    "                    self.word_dict[w]=0\n",
    "\n",
    "                if tag not in self.tag_dict:\n",
    "                    self.tag_dict[tag]=0\n",
    "\n",
    "                self.word_dict[w]+=1\n",
    "                self.tag_dict[tag]+=1\n",
    "   \n",
    "        for t in self.tag_dict:\n",
    "            total += self.tag_dict[t]\n",
    "    \n",
    "        self.total_tags = total\n",
    "        for sent in train_data:\n",
    "            for i in range(len(sent)):\n",
    "                word = sent[i]\n",
    "                \n",
    "                if i == 0:\n",
    "                    if word[1] not in self.S:\n",
    "                        self.S[word[1]] = 1\n",
    "                    else:\n",
    "                        self.S[word[1]] += 1\n",
    "\n",
    "                if i == len(sent)-1:\n",
    "                    if (word[1],word[0]) not in self.O:\n",
    "                        self.O[(word[1],word[0])] = 1\n",
    "                    else:\n",
    "                        self.O[(word[1],word[0])] += 1\n",
    "                    continue                \n",
    "                \n",
    "                word2 = sent[i+1]\n",
    "                \n",
    "                if (word[1],word2[1]) not in self.P:\n",
    "                    self.P[(word[1],word2[1])] = 1\n",
    "                else:\n",
    "                    self.P[(word[1],word2[1])] += 1\n",
    "                \n",
    "                if (word[1],word[0]) not in self.O:\n",
    "                    self.O[(word[1],word[0])] = 1\n",
    "                else:\n",
    "                    self.O[(word[1],word[0])] += 1\n",
    "                    \n",
    "        for (s,s_1) in self.P:\n",
    "            self.P[(s,s_1)] /= float(self.tag_dict[s])\n",
    "\n",
    "        for (s,v) in self.O:\n",
    "            self.O[(s,v)] /= float(self.tag_dict[s])\n",
    "\n",
    "        start_sum = 0\n",
    "        for s in self.S:\n",
    "            start_sum += self.S[s]\n",
    "        self.start_sum = start_sum\n",
    "        for s in self.S:\n",
    "            self.S[s] /= float(start_sum)\n",
    "            \n",
    "            \n",
    "    def transition(self,s1,s2):\n",
    "        if (s1,s2) not in self.P:\n",
    "            return self.laplace_constant/(self.laplace_constant*(self.total_tags) + self.queryTag(s1))\n",
    "            #return 10e-10\n",
    "        else:\n",
    "            return self.P[(s1,s2)]\n",
    "    \n",
    "    def emission(self,s1,v1):\n",
    "        if (s1,v1) not in self.O:\n",
    "            return self.laplace_constant/(self.laplace_constant*(self.total_tags) + self.queryTag(s1))\n",
    "            #return 10e-10\n",
    "        else:\n",
    "            return self.O[(s1,v1)]\n",
    "        \n",
    "    def start(self,s):\n",
    "        if s not in self.S:\n",
    "            return 1.0/(self.start_sum)\n",
    "            #return 10e-10\n",
    "        else:\n",
    "            return self.S[s]\n",
    "        \n",
    "    def getTags(self):\n",
    "        return self.tag_dict\n",
    "    \n",
    "    def getWords(self):\n",
    "        return self.word_dict\n",
    "    \n",
    "    def queryTag(self,s):\n",
    "        if s not in self.tag_dict:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.tag_dict[s]\n",
    "        \n",
    "    def test(self,test_data):\n",
    "        \n",
    "        tag_set = self.tag_dict.keys()\n",
    "        predictions = []\n",
    "        \n",
    "        for sent in test_data:\n",
    "            # Assignment Task 2 - Sub Part 2 Viterbi algorithm\n",
    "            \n",
    "            V = {}\n",
    "            V_1 = {}\n",
    "\n",
    "            maxi = 0\n",
    "            decisions = []\n",
    "\n",
    "            for t in tag_set:\n",
    "                V[t] = math.log(self.start(t)) + math.log(self.emission(t,sent[0][0]))\n",
    "\n",
    "            for i in range(1,len(sent)):        \n",
    "                word = sent[i][0]\n",
    "                decision = {}\n",
    "                for t in tag_set:\n",
    "                    maxi = None\n",
    "                    for t2 in tag_set:\n",
    "                        value = math.log(self.emission(t,word)) + V[t2] + math.log(self.transition(t2,t)) \n",
    "                        if maxi == None:\n",
    "                            maxi = value\n",
    "                            decision[t] = t2\n",
    "                        if value > maxi:\n",
    "                            maxi = value\n",
    "                            decision[t] = t2\n",
    "                    V_1[t] = maxi\n",
    "\n",
    "                decisions.append(decision)         \n",
    "                V = V_1.copy()\n",
    "                \n",
    "            maxi = None\n",
    "            dec = None\n",
    "\n",
    "            for t in tag_set:\n",
    "                if maxi == None:\n",
    "                    maxi = V[t]\n",
    "                    dec = t\n",
    "                if V[t] > maxi:\n",
    "                    maxi = V[t]\n",
    "                    dec = t\n",
    "\n",
    "            bestSeq = []\n",
    "            bestSeq.append(dec)\n",
    "\n",
    "            for i in range(len(decisions)-1,-1,-1):\n",
    "                dec = decisions[i][dec]\n",
    "                bestSeq.append(dec)\n",
    "\n",
    "            bestSeq.reverse()\n",
    "            predictions.append(bestSeq)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sent):\n",
    "    return [label for i,label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=[sent2labels(s) for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HMM()\n",
    "model.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9414225941422594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9391370192817936"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assignment Task 2 - Sub Part 3\n",
    "\n",
    "y_predict = model.test(test_data)\n",
    "#print(type(y_predict))\n",
    "\n",
    "flat_y_test = [t for s in y_test for t in s]\n",
    "flat_y_predict = [t for s in y_predict for t in s]\n",
    "print(\"Accuracy: \",accuracy_score(flat_y_test,flat_y_predict, normalize=True))\n",
    "\n",
    "\n",
    "labels = []\n",
    "for t in model.getTags():\n",
    "    labels.append(t)\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_predict, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          .      0.971     1.000     0.985        33\n",
      "          X      1.000     0.333     0.500         3\n",
      "        ADJ      0.938     0.833     0.882        18\n",
      "        ADP      0.929     0.963     0.945        27\n",
      "        ADV      0.889     0.889     0.889         9\n",
      "       VERB      0.941     0.914     0.928        35\n",
      "        DET      0.971     1.000     0.985        33\n",
      "       CONJ      1.000     1.000     1.000         7\n",
      "       NOUN      0.904     0.922     0.913        51\n",
      "       PRON      1.000     1.000     1.000        12\n",
      "        PRT      0.917     1.000     0.957        11\n",
      "        NUM      0.000     0.000     0.000         0\n",
      "\n",
      "avg / total      0.942     0.941     0.939       239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_predict, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Assignment Task 3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent,i):\n",
    "    word = sent[i][0]\n",
    "    \n",
    "    features ={\n",
    "    'bias': 1.0, # for stochasticity \n",
    "    'first_word': i == 0 ,# Because a sentence can start with only limited words from the corpus\n",
    "    'last_word' : i == (len(sent)-1) ,# Because a sentence can end with only limited words from the corpus\n",
    "    'word.lower()': word.lower(), # the actual work \n",
    "    'word[-3:]': word[-3:], # last 2 letters for checking verb forms and adjectives\n",
    "    'word[-2:]': word[-2:], # last 2 letters for checking verb forms and adjectives\n",
    "    'word.isupper()': word.isupper(), # for checking acronyms\n",
    "    'word.istitle()': word.istitle(), # for checking propernouns\n",
    "    'word.isdigit()': word.isdigit(), # for checking numbers\n",
    "    }\n",
    "                \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent,i) for i in range(len(sent))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[sent2features(s) for s in train_data]\n",
    "y_train=[sent2labels(s) for s in train_data]\n",
    "\n",
    "X_test=[sent2features(s) for s in test_data]\n",
    "y_test=[sent2labels(s) for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.1, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'NOUN', 'ADJ', 'VERB', 'ADP', '.', 'ADV', 'CONJ', 'PRT', 'PRON', 'NUM', 'X']\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9601358414424452"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "labels=list(crf.classes_)\n",
    "print(labels)\n",
    "print(type(labels))\n",
    "metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          .      1.000     1.000     1.000        33\n",
      "          X      0.000     0.000     0.000         3\n",
      "        ADJ      1.000     0.833     0.909        18\n",
      "        ADP      0.963     0.963     0.963        27\n",
      "        ADV      0.900     1.000     0.947         9\n",
      "       VERB      0.972     1.000     0.986        35\n",
      "        DET      1.000     1.000     1.000        33\n",
      "       CONJ      1.000     1.000     1.000         7\n",
      "       NOUN      0.927     1.000     0.962        51\n",
      "       PRON      1.000     0.917     0.957        12\n",
      "        PRT      0.917     1.000     0.957        11\n",
      "        NUM      0.000     0.000     0.000         0\n",
      "\n",
      "avg / total      0.956     0.967     0.960       239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/du0/15CS30008/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "########### BONUS ############\n",
    "# Hyper-parameters tuning\n",
    "########## NOTE ############## \n",
    "# Set the no of n_jobs to less than the number of cores available in the computer #\n",
    "\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=48,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = rs.best_estimator_\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT Justifying the hyperparameters found\n",
    "\n",
    "_x = [s.parameters['c1'] for s in rs.grid_scores_]\n",
    "_y = [s.parameters['c2'] for s in rs.grid_scores_]\n",
    "_c = [s.mean_validation_score for s in rs.grid_scores_]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 12)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('C1')\n",
    "ax.set_ylabel('C2')\n",
    "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
    "    min(_c), max(_c)\n",
    "))\n",
    "\n",
    "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
